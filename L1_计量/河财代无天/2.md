
### 0.1.1 核心问题与概念汇总

| 主要问题/概念 | 核心内容 |
| :--- | :--- |
| **普通最小二乘法 (OLS)** | 一种参数估计方法，其核心准则是使模型的**残差平方和 (RSS) 最小化**。 |
| **拟合优度** | 衡量样本回归函数对样本数据拟合程度的指标，如 $R^2$（可决系数）和调整的 $R^2$。 |
| **由样本推断总体** | 使用样本回归函数的参数估计值及其统计量，来检验总体参数的显著性并估计其置信区间。 |
| **参数的经济含义** | 在其他解释变量不变时，某解释变量每增加一个单位，被解释变量的**平均**变化量。 |
| **OLS 的结论** | OLS回归具有一系列数学性质，其中最常考的是：**样本均值点 $(\bar{x}, \bar{y})$ 必定落在样本回归线上**。 |
| **OLS 的基本假定** | 为确保OLS估计量具有优良性质而必须满足的一系列前提条件，如零均值、同方差、无自相关等。 |
| **高斯-马尔可夫定理** | 在基本假定下，OLS估计量是所有线性无偏估计量中方差最小的，即**最佳线性无偏估计量 (BLUE)**。 |
| **估计量的性质** | - **小样本性质**: 线性性、无偏性、最小方差性。<br>- **大样本性质**: 一致性。 |
| **随机误差项方差的估计** | 估计随机误差项的方差 ($\sigma^2$) 与标准差 ($\sigma$)，后者也称为**回归标准差**。 |

---

## 0.2 【计量经济学】期末过关攻略 - 第二、三章精讲

[原视频链接](https://www.bilibili.com/video/BV1QJ411h7YX/)

### 0.2.1 核心内容框架

1.  **普通最小二乘法 (OLS)**：定义和性质。
2.  **拟合优度**：
    *   一元回归（单个解释变量$x$）：使用 **可决系数 $R^2$**。
    *   多元回归（多个解释变量$x$）：使用 **调整的 $R^2$**。
3.  **由样本推断总体**：
    *   **核心思路**：
        > 由样本函数的参数 $\hat{\beta}_j$ 及其统计量，去推断总体函数的参数 $\beta_j$ 是否显著。若显著，则求其置信区间。
    *   **是否显著**：检验 $\beta_j$ 是否等于零。
        *   $\beta_j \neq 0$：显著，意味着对应的解释变量 $x_j$ 对 $y$ 有显著影响。
        *   $\beta_j = 0$：不显著。
    *   **检验方法**：显著性检验。
    *   **求范围**：置信区间估计。
4.  **点预测和区间预测**。



### 0.2.2 一、普通最小二乘法 (OLS) 的定义与性质

#### 0.2.2.1 OLS的定义

OLS是一种参数估计方法，其核心准则是：**使得模型的残差平方和 (Sum of Squared Residuals, SSR 或 RSS) 达到最小值**。

*   **残差 ($e_i$)**：观测值与拟合值之差。
    > 残差 = 观测值 ($Y_i$) - 拟合值 ($\hat{Y}_i$)
*   **残差平方和 (RSS)**：先将每个残差进行平方，然后再求和。
    $$
    RSS = \sum e_i^2 = \sum (Y_i - \hat{Y}_i)^2
    $$
*   **OLS估计量**：使上述RSS最小化所求得的参数（如 $\hat{\beta}_0, \hat{\beta}_1$）。
*   **OLS法**：以“最小化残差平方和”为准则来求解参数的过程。

> **考点1：OLS的定义（最小二乘准则）**
>
> *   **例题**：（第二套卷子第三题）最小二乘准则是指参数的估计量应使：
> *   **分析**：
>     *   A. $\sum(Y_i - \hat{Y}_i)$ 最小：错误，这是残差之和，且没有平方。
>     *   B. $\sum(Y_i - \hat{Y}_i)^2$ 最小：错误，这是残差的和再平方，顺序反了。
>     *   **C. $\sum(Y_i - \hat{Y}_i)^2$ 最小**：正确，这才是**残差平方和**。先算残差($Y_i - \hat{Y}_i$)，再平方，最后求和。

#### 0.2.2.2 解释参数的经济含义

> **考点2：解释参数的经济含义**
>
> *   **通用模板**：
>     > 在**其他解释变量不变**的情况下，（某解释变量）$x_k$ 每增加一个单位，（被解释变量）$y$ **平均**增加（或减少）$\beta_k$ 个单位。
>
> *   **例题**：研究印度$30$个农户家庭的食物支出($y$)和总收入($x$)的关系，得到如下结果：

| Variable | Coefficient |
| :------- | :---------- |
| C        | $94.2118$   |
| X        | $0.4376$    |
>
> *   **步骤1：写出样本回归函数 (SRF)**
>     *   这里的 `C` 代表常数项 $\hat{\beta}_0$，`X` 代表变量 $x$ 的系数 $\hat{\beta}_1$。
>     *   保留两位小数，样本回归函数为：
>         $$
>         \hat{y} = 94.21 + 0.44x
>         $$
> *   **步骤2：解释斜率系数的经济含义**
>     *   解释 $\hat{\beta}_1 = 0.44$ 的含义：
>         > 在其他解释变量（本例中没有）不变的情况下，**总收入** ($x$) 每增加$1$单位，**食物支出** ($y$) **平均**增加 $0.44$ 单位。
>     *   **注意**：务必包含“**平均**”二字。

#### 0.2.2.3 OLS的结论

OLS方法本身会带来一些固有的数学结论（共$6$条，见书本P$24$），重在记忆，不需深究推导。

> **考点3：OLS结论的直接考察**
>
> *   **最常考的结论**：
>     > **样本均值点 $(\bar{x}, \bar{y})$ 必定落在样本回归线上。**
> *   **解释**：
>     1.  根据样本数据 $(x_i, y_i)$，我们可以计算出 $x$ 的均值 $\bar{x}$ 和 $y$ 的均值 $\bar{y}$，构成一个点 $(\bar{x}, \bar{y})$。
>     2.  同时，我们用这些样本数据通过OLS法得到一条样本回归线 $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$。
>     3.  这个样本均值点 $(\bar{x}, \bar{y})$ 一定会精确地落在这条回归线上。

---

### 0.2.3 二、OLS 估计量的性质 (高斯-马尔可夫定理)

OLS估计量要拥有优良的性质，需要一个前提：**模型满足一系列基本假定**。

> **核心逻辑**：
> **普通最小二乘法 (OLS) + 基本假定 $\implies$ OLS估计量具有优良性质**

#### 0.2.3.1 基本假定

这些假定是确保OLS估计量好用（如无偏、有效）的前提，考试主要以名词形式出现。

1.  **解释变量 $X$ 是非随机的**：$X$ 是确定性变量或可控变量（如实验中设定的剂量、政府政策等）。
2.  **随机误差项的零均值性**：对于给定的 $X$ 值，误差项的期望（均值）为 $0$。
    $$
    E(u_i | X_i) = 0
    $$
3.  **同方差性**：对于给定的 $X$ 值，所有误差项 $u_i$ 的方差都相同，为一个常数 $\sigma^2$。
    $$
    Var(u_i | X_i) = \sigma^2
    $$
4.  **无自相关性**：不同观测值的误差项之间不相关。
    $$
    Cov(u_i, u_j) = 0 \quad (i \neq j)
    $$
5.  **随机误差项服从正态分布**：$u_i$ 服从均值为$0$，方差为 $\sigma^2$ 的正态分布。
    $$
    u_i \sim N(0, \sigma^2)
    $$
6.  **解释变量矩阵 $X$ 是满秩的**：（记住即可）
7.  **解释变量的离差平方和收敛于一个常数**：（记住即可）

#### 0.2.3.2 估计量的优良性质

##### 0.2.3.2.1 (1) 高斯-马尔可夫定理 (Gauss-Markov Theorem)

> 在**基本假定 1-4** 成立的条件下，**普通最小二乘估计量 (OLS Estimator)** 是所有**线性无偏估计量**中方差最小的估计量，即**最佳线性无偏估计量 (Best Linear Unbiased Estimator, BLUE)**。

*   **线性性 (Linearity)**：指OLS估计量 $\hat{\beta}_j$ 是被解释变量 $Y_i$ 的线性函数。
*   **无偏性 (Unbiasedness)**：指OLS估计量的期望值等于总体的真实参数值，即 $E(\hat{\beta}_j) = \beta_j$。
    *   **通俗理解**：如果我们用不同的样本数据反复进行抽样和估计，得到的许多 $\hat{\beta}_j$ 值的平均值会等于真实的 $\beta_j$。
*   **最小方差性 (Minimum Variance / Best)**：在线性和无偏的所有估计方法中，OLS法得到的估计量 $\hat{\beta}_j$ 的方差是最小的。
    *   **通俗理解**：OLS估计量更紧密地分布在真实的 $\beta_j$ 周围，估计结果更稳定。

**这三个性质（线性、无偏、最小方差）合称为小样本性质。**

##### 0.2.3.2.2 (2) 一致性 (Consistency)

> 随着**样本容量 $n$ 增大**，OLS估计量 $\hat{\beta}_j$ 会越来越接近总体的真实参数 $\beta_j$。

*   这是一个**大样本性质**，因为它依赖于样本容量 $n$ 趋向于无穷大。
*   **通俗理解**：数据越多，估计结果越准。

---

### 0.2.4 三、随机误差项方差 ($\sigma^2$) 的估计

我们不仅可以估计参数 $\beta_j$，还可以估计随机误差项 $u$ 的方差 $\sigma^2$ 及其标准差 $\sigma$。

*   **随机误差项方差的估计量** ($\hat{\sigma}^2$)：
    $$
    \hat{\sigma}^2 = \frac{\sum e_i^2}{n-k-1} = \frac{RSS}{n-k-1}
    $$
    其中：
    *   $RSS = \sum e_i^2$：残差平方和。
    *   $n$：样本容量。
    *   $k$：解释变量的个数。

*   **回归标准差 (Standard Error of Regression, SER)** ($\hat{\sigma}$):
    *   它是随机误差项方差估计量的平方根，也叫**随机误差项的标准差的估计量**。
    $$
    \hat{\sigma} = \sqrt{\hat{\sigma}^2} = \sqrt{\frac{RSS}{n-k-1}}
    $$

> **考点4：计算回归标准差**
>
> *   **例题**：（第二套卷子判断题第二题）依据样本容量为 $102$ 的样本数据，利用OLS估计一个含截距项的一元线性回归模型。如果残差平方和为 $400$，那么回归标准差等于 $2$。
> *   **分析**：
>     1.  **提取信息**：
>         *   样本容量 $n = 102$。
>         *   一元线性回归模型 $\implies$ 解释变量个数 $k=1$。
>         *   残差平方和 $RSS = 400$。
>     2.  **计算随机误差项方差的估计值 $\hat{\sigma}^2$**：
>         $$
>         \hat{\sigma}^2 = \frac{RSS}{n-k-1} = \frac{400}{102 - 1 - 1} = \frac{400}{100} = 4
>         $$
>     3.  **计算回归标准差 $\hat{\sigma}$**：
>         $$
>         \hat{\sigma} = \sqrt{\hat{\sigma}^2} = \sqrt{4} = 2
>         $$
>     4.  **结论**：题目中的说法“回归标准差等于$2$”是**正确**的。


### **15分钟教会你数据清洗（使用SQL对数据进行清洗）**
> **UP主**: 热心网友灵灵七
> **发布时间**: 2025-06-10 12:02:31
> **原始链接**: [https://www.bilibili.com/video/BV1CQT6zsE5V/](https://www.bilibili.com/video/BV1CQT6zsE5V/)

---

### 本期内容聚焦问题汇总

| 序号 | 问题 |
| :-- | :--- |
| $1$ | 数据清洗到底要遵循哪些原则？ |
| $2$ | 具体怎么用 SQL 这个武器来清洗数据？ |
| $3$ | 清洗完之后怎么让数据自己说话（可视化）？ |

---

### 正文笔记

> 别看这标题挺朴实, 但里面的门道可不少啊, 在深入数据清洗之前, 咱们先快速回顾一下数据处理的两大阵营, OLTP 和 OLAP, 你可以把他们想象成两个性格迥异的兄弟老大, OLTP 就是那个雷厉风行处理日常事务的, 比如你网购下单, 银行转账, 这些实时性要求高的操作都归他管, 它的核心任务是保证数据的准确, 及时写入和读取, 保证系统稳定运行, 像我们之前聊的 SQL 优化, 事务控制都是他的拿手好戏, 而老 $2$ OLAP 则是个慢条斯理, 喜欢思考的家伙, 他不关心数据什么时候存进去, 而是关注已经存进去的数据能告诉我们什么, 他负责从海量数据里挖掘规律, 生成报表, 给决策提供依据, 想想看你公司每天产生那么多订单, 用户行为数据堆在那, 光有数据没用, 得分析出来才能指导业务, 但问题来了, 这些数据仓库里的数据往往不是那么干净, 可能有重复的, 缺胳膊少腿的单位还不统一的, 这时候 OLAP 的第一步也是最关键的一步, 就是数据清洗。
>
> 为什么说数据质量是王道, 你想啊, 你辛辛苦苦建了个模型, 结果发现预测结果一塌糊涂, 是不是很崩溃, 很多时候不是模型不行, 而是喂给模型的粮食, 数据太差了, 就像你用最顶级的食材, 结果厨师是个新手, 做出来的菜味道也一般, 数据质量决定了你能挖掘出多少信息, 再牛的算法也得受制于数据本身, 所以高质量的数据清洗才能有高质量的数据, 这是咱们今天讨论的核心。

今天咱们就聚焦三个问题, 第一数据清洗到底要遵循哪些原则, 第二具体怎么用 SQL 这个武器来清洗数据, 第三清洗完之后怎么让数据自己说话, 也就是可视化。

#### **一、数据清洗的原则：完全合一**

> 说到数据清洗的原则, 我之前在数据分析实战 $45$ 讲里提过, 这里再用一个经典案例, 泰坦尼克号数据集给大家掰扯掰扯, 这数据集大家肯定不陌生, 预测谁活下来了, 谁没活下来, 但拿到原始数据, 你会发现它就像个淘气包, 总有各种各样的小毛病, 比如年龄 `age`, 这里空了一大片船舱号, `cabin` 更是稀稀拉拉, 还有些地方单位可能都不统一, 甚至有些数值看着就让人怀疑人生, 面对这么多问题, 我们不能头痛医头, 脚痛医脚得有个章法, 我把这些原则总结成了四个字, **完全合一**。

##### **“完全合一”准则**
听起来有点玄乎, 其实就是四个维度：
1.  **完整性 (Completeness)**：看数据是不是完整，有没有缺胳膊少腿的，也就是检查空值。
2.  **全面性 (Comprehensiveness)**：要看每个字段是不是都符合他的角色定位，比如年龄应该是数字，性别应该是男或女，而不是乱七八糟的字符。还要看看数据类型是不是合适，比如乘客 ID 用整数比用字符串更方便后续处理。
3.  **合法性 (Legality)**：就是检查数据内容是不是靠谱，比如年龄不可能是负数吧，船票等级只能是 $1$、$2$、$3$ 吧，票价也不能高到天上去了。
4.  **唯一性 (Uniqueness)**：就是要确保每条记录都是独一无二的，不能有重复。比如如果乘客 ID 是主键，那肯定不能有两个乘客有相同的 ID。

##### **案例背景：泰坦尼克号数据集**

> 咱们先简单认识一下今天的主角, 泰坦尼克号数据集, 它主要包含两个文件, `train.csv` 和 `test.csv`。训练集 `train.csv` 里, 既有乘客的各种信息, 比如姓名、性别、年龄、票价等等, 还有一个关键的标签, `survived` 告诉你这位乘客到底活没活下来。测试集 `test.csv` 呢只给了特征, 没有标签, 就是让你根据训练集学到的本事, 去预测这些乘客的命运。今天我们不搞复杂的模型预测, 重点是看看在正式分析之前, 这堆数据到底脏在哪, 我们该怎么用 SQL 把它洗干净。
>
> 大家可以先对这些字段有个印象, 比如 `pclass` 代表仓位等级, 一等舱最贵, 三等舱最便宜, `sibsp` 和 `parch` 是关于亲戚数量的, `embarked` 是登船港口。
>
> 好, 现在我们把数据导入到 MYSQL 里, 用 NAVCAT 这种工具很方便, 随便瞅几眼, 是不是感觉有点眼花缭乱, 特别是 `age` 和 `cabin` 这两列, 好多地方写着 `NULL`, 这就是典型的缺失值。这就像你点了一份外卖, 结果发现菜单上有些菜不见了, 或者配料表上有些东西没写, 让人心里直打鼓。
>
> 除了缺失值, 我们还得想想其他地方有没有问题, 比如年龄会不会出现负数, 票价会不会高的离谱, 同一个人会不会出现两次记录, 这些都是我们需要检查的潜在雷区。
>
> 这四个原则是通用的, 但具体到不同的数据集, 问题可能千奇百怪, 就像刚才提到的 steam 游戏例子, 用户购买记录表里用游戏名称来关联游戏表, 虽然也能用, 但效率不高, 而且容易出错, 如果我们能把游戏表里的 ID 直接关联过来, 不仅查询快, 还能更清晰的看出数据是否完整, 这就是对症下药的意思。

---

#### **二、实战：用 SQL 清洗泰坦尼克号数据**

> 理论说了不少, 现在咱们真刀真枪的用 SQL 来清洗, 泰坦尼克号的数据, 目标很明确, 就是把 `titanic_train` 这张表里的数据捋顺了, 按照刚才的完全合一原则, 我们大致分几步走：
> 1.  **第一步**：检查完整性，看看哪些字段有空值，有多少个空值。
> 2.  **第二步**：针对发现的空值采取相应的处理策略，比如删掉、填平均值或者填最常见的值。
> 3.  **第三步**：检查全面性，看看字段的数据类型是不是都合适，比如年龄是不是整数，票价是不是小数。
> 4.  **第四步**：最后再检查一下合法性和唯一性，确保数据没有明显的错误和重复。
>
> 整个过程我们会用到 MYSQL 数据库, 配合 NAVCAT 或者 `mysql for excel` 这样的工具来操作。

##### **第一步：检查完整性，统计空值**

*   **简单方法：逐字段检查**
    > 最简单粗暴的方法就是一条一条字段去查, 比如想知道年龄 `age` 有多少个空值, 直接写个 `select count(*) from titanic_train where age is null`。
    ```sql
    SELECT COUNT(*) FROM titanic_train WHERE Age IS NULL;
    ```
    > 跑一下, 结果出来了, $177$ 个, 简单明了。如果你想同时看两个字段, 比如 `age` 和 `cabin`, 可以用 `CASE WHEN` 语句把每个字段的空值计数算出来。
    ```sql
    SELECT 
        SUM(CASE WHEN Age IS NULL THEN 1 ELSE 0 END) AS age_null_count,
        SUM(CASE WHEN Cabin IS NULL THEN 1 ELSE 0 END) AS cabin_null_count
    FROM titanic_train;
    ```
    > 你看 `age` 还是 $177$, `cabin` 居然有 $687$ 个, 这比例可不小啊。不过这种方法有个缺点, 如果表里有几十个字段, 你得手动写几十个 `CASE WHEN`, 想想都头大, 而且一不小心就容易写错。

*   **高级方法：使用存储过程自动化检查**
    > 那有没有更聪明的办法呢? 当然有, 当字段一多就得靠程序来帮忙了, 在 MYSQL 里我们可以用存储过程来实现自动化检查。思路其实不复杂：
    > 1.  第一步：先去 `information_schema.columns` 这个系统表里, 把我们要检查的表的所有列名都找出来。这个表就像是数据库的字典, 记录了所有表的结构信息。
    > 2.  第二步：用游标 (Cursor) 这个东西, 就像一个指针, 一个一个的遍历这些列名。
    > 3.  第三步：拿到每个列名后, 我们把它拼接到一个 SQL 查询语句里, 比如 `select count(*) where [列名] is null`, 然后执行这个动态生成的语句。
    >
    > 这样程序就能自动帮我们, 检查完所有字段的空值情况了, 是不是很方便, 这就像给数据做了一次全面的体检, 效率高, 还不容易出错。
    >
    > 这就是那个存储过程的代码, 看起来有点复杂, 但拆开来看就明白了, 我们定义了一个存储过程, 需要传入数据库名和表明, 然后我们声明了一个变量 `temp_column` 来存放当前处理的列名, 还有一个 `done` 标志位, 用来判断循环是否结束。关键在于 `DECLARE cursor_column CURSOR FOR` 这一句, 他告诉 MYSQL, 我们要从 `information_schema.columns` 里取出所有属于指定表的列名, 并用游标 `cursor_column` 来逐个读取。接着我们用 `LOOP` 循环 `FETCH cursor_column INTO temp_column`, 把下一个列名读到 `temp_column` 里, 如果 `done` 为真, 说明所有列都处理完了, 就跳出循环。循环体里, 最重要的就是 `SET @temp_query = CONCAT(...)` 这里用 `CONCAT` 函数把 SQL 语句动态拼接起来, 注意 `WHERE` 后面的列名是 `temp_column`, `COUNT` 的结果, 别名也用了 `temp_column` 加上 `_null_count`, 这样结果就能清晰地显示哪个列的空值数。`PREPARE stmt FROM @temp_query; EXECUTE stmt;` 这两句是执行这个动态生成的 SQL 语句。

    ```sql
    DELIMITER $$
    CREATE PROCEDURE `check_null_values`(IN db_name VARCHAR(255), IN tbl_name VARCHAR(255))
    BEGIN
        DECLARE done INT DEFAULT FALSE;
        DECLARE temp_column VARCHAR(255);
        
        -- 声明游标，用于遍历指定表的所有列名
        DECLARE cursor_column CURSOR FOR 
            SELECT column_name 
            FROM information_schema.columns
            WHERE table_schema = db_name AND table_name = tbl_name;
            
        -- 声明一个 continue handler，当没有更多行可 fetch 时，设置 done 为 TRUE
        DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
        
        -- 打开游标
        OPEN cursor_column;
        
        read_loop: LOOP
            -- 从游标中获取下一列名
            FETCH cursor_column INTO temp_column;
            
            -- 如果没有更多列，则退出循环
            IF done THEN
                LEAVE read_loop;
            END IF;
            
            -- 动态构建 SQL 查询语句，用于统计当前列的空值数量
            SET @temp_query = CONCAT(
                'SELECT \'', temp_column, '\' AS column_name, ',
                'COUNT(*) AS null_count ',
                'FROM `', tbl_name, '` WHERE `', temp_column, '` IS NULL OR `', temp_column, '` = \'\';'
            );
            
            -- 准备并执行动态 SQL
            PREPARE stmt FROM @temp_query;
            EXECUTE stmt;
            DEALLOCATE PREPARE stmt;
            
        END LOOP;
        
        -- 关闭游标
        CLOSE cursor_column;
    END $$
    DELIMITER ;
    ```

*   **检查结果**
    > 运行这个存储过程, 输入我们的数据库名和表明 `titanic_train`, 它就会返回一堆结果, 为了看得清楚, 我们整理一下, 就能看到这张表里, `PassengerId`, `Survived`, `Pclass` 这些字段都是零个空值, 说明它们是完整的, 但是 `Age` 有 $177$ 个, `Cabin` 有 $687$ 个, `Embarked` 有 $2$ 个, 这就印证了我们之前的观察确实存在缺失值, 特别是 `Cabin` 超过一半的记录都没有船舱号信息, 这在分析时可能会是个问题。

##### **第二步：处理空值**

> 好了, 知道了哪些字段有空值, 下一步就是决定怎么处理他们, 面对空值, 我们通常有三种选择：
> 1.  **直接删掉**：如果某条记录的某个字段是空的, 干脆把整条记录删掉。这招简单粗暴, 但缺点也很明显, 万一这个缺失值很重要, 或者缺失的比例很高, 删掉之后数据量就少了, 甚至可能引入偏差。比如如果年龄缺失的乘客, 恰好都是某个特定群体, 那删掉他们就可能影响分析结果。
> 2.  **填充**：用某种方法把空值补上。最常用的是用**均值**, 也就是这个字段所有非空值的平均数来填, 也可以用**中位数**或者**众数**。这种方法的好处是保留了更多的数据, 但坏处是可能会扭曲数据的分布, 比如用均值填充, 可能会让数据的方差变小。
> 3.  **高频填充**：对于分类变量, 比如性别, 如果缺失值比较少, 可以用出现频率最高的那个值来填。比如如果男性比女性多, 就把缺失的性别都填成男性。这在某些情况下是合理的, 但同样可能引入偏差。

*   **处理 `Age` 字段（均值填充）**
    > 对于 `age` 字段, 我们决定用均值填充。但直接写 `UPDATE titanic_train SET age = (SELECT AVG(age) FROM titanic_train WHERE age IS NOT NULL) WHERE age IS NULL;` MYSQL 会报错, 提示你不能在同一个 `UPDATE` 语句里, 同时查询和修改同一个表, 这是数据库为了防止意外操作而设置的限制。
    > 怎么办呢? 一个常见的技巧是创建一个临时表, 先把原表的数据复制到一个新的临时表里, 比如叫 `titanic_train2`, 然后我们在更新原表 `titanic_train` 的时候, 去计算临时表 `titanic_train2` 的平均年龄, 再把这个值填到原表的空值里, 这样就绕开了那个限制。

    ```sql
    -- 1. 创建一个临时表并复制数据
    CREATE TABLE titanic_train_temp LIKE titanic_train;
    INSERT INTO titanic_train_temp SELECT * FROM titanic_train;

    -- 2. 使用临时表计算的平均值来更新原表
    UPDATE titanic_train
    SET Age = (SELECT AVG(Age) FROM titanic_train_temp WHERE Age IS NOT NULL)
    WHERE Age IS NULL;

    -- 这里还用了ROUND函数, 把平均值四舍五入, 保留了一位小数, 让结果更整洁一些
    UPDATE titanic_train
    SET Age = (SELECT ROUND(AVG(Age), 1) FROM titanic_train_temp WHERE Age IS NOT NULL)
    WHERE Age IS NULL;

    -- 3. 删除临时表
    DROP TABLE titanic_train_temp;
    ```

*   **处理 `Cabin` 字段（不处理）**
    > 接下来是 `cabin` 字段, 空值最多有 $687$ 个, 这可怎么办。我们先看看这个字段的分布情况。
    ```sql
    SELECT COUNT(Cabin) FROM titanic_train; -- 结果是 204
    SELECT COUNT(DISTINCT Cabin) FROM titanic_train; -- 结果是 147
    ```
    > `COUNT(Cabin)` 是 $204$, 说明总共有 $204$ 个非空的船舱号记录, 而 `COUNT(DISTINCT Cabin)` 是 $147$, 说明有 $147$ 种不同的船舱号。这说明什么, 说明船舱号的种类非常多, 而且很多乘客可能住在同一个舱位。再结合常识, 想想船舱位置对生存率的影响可能很大, 但缺失这么多, 直接删掉肯定不行, 填充均值或者众数也不太合适, 因为船舱号本身就是个分类变量, 而且分布太广了。这时候一个比较务实的做法就是**不处理**。既然无法有效填充, 而且这些缺失值本身, 可能并不代表什么特殊信息, 或者对分析结果影响不大, 那就干脆保留它们, 让它们保持 `NULL` 状态。当然这取决于你的具体分析目标。

*   **处理 `Embarked` 字段（高频值填充）**
    > 最后是 `embarked` 的字段, 只有两个空值。这俩兄弟是谁呢, 我们先看看各个港口的乘客数量。
    ```sql
    SELECT Embarked, COUNT(*) 
    FROM titanic_train 
    GROUP BY Embarked;
    ```
    > `Southampton` 最多有 $644$ 人, `Cherbourg` 次之 $168$ 人, `Queenstown` 最少 $77$ 人。那两个空值的乘客, 很可能也是从 `Southampton` 出发的, 毕竟那里是绝大多数人的起点。所以我们用 `Southampton` (简写 `S`) 来填充这两个缺失值。
    ```sql
    UPDATE titanic_train
    SET Embarked = 'S'
    WHERE Embarked IS NULL OR Embarked = '';
    ```
    > 这样一来, `titanic_train` 表里的所有空值问题都解决了, `Age` 用了均值填充, `Cabin` 保留了缺失值, `Embarked` 用了高频值填充。

##### **第三步：检查全面性，规范数据类型**

> 解决了空值问题, 我们进入第二步检查全面性。还记得我们是怎么把数据导入 MYSQL 的吗, 是从 CSV 文件直接导入的, 这种导入方式有个默认行为, 就是把所有字段都当成 `varchar` 类型, 长度通常是 $255$。但这显然不合理, 比如乘客 ID、生存状态、仓位等级、亲戚数量, 这些明明都是数字, 干嘛用字符串存呢? 还有年龄和票价, 用 `decimal` 类型更合适, 可以精确表示小数。
>
> 所以我们需要把这些字段的类型改过来。像 `PassengerId`, `Survived`, `Pclass`, `SibSp`, `Parch` 这些改成 `INT` 类型。`Age` 和 `Fare` 改成 `DECIMAL` 类型。`Name`, `Ticket`, `Cabin` 这些文本信息可以保持 `varchar`, 但可能需要调整长度。`Sex` 和 `Embarked` 这两个字段可以用 `varchar`, 也可以考虑用 `ENUM` 枚举类型限制, 只能填预设的几个值, 这样更规范也更省内存。
>
> 修改字段类型, 用 `ALTER TABLE ... CHANGE` 语句就行。

```sql
ALTER TABLE titanic_train
    -- 将 PassengerId 改为 INT，并设为主键
    CHANGE PassengerId PassengerId INT(11) NOT NULL PRIMARY KEY,
    -- 将 Survived, Pclass, SibSp, Parch 改为 INT
    CHANGE Survived Survived INT(11) NOT NULL,
    CHANGE Pclass Pclass INT(11) NOT NULL,
    CHANGE SibSp SibSp INT(11) NOT NULL,
    CHANGE Parch Parch INT(11) NOT NULL,
    -- 将 Age 和 Fare 改为 DECIMAL
    CHANGE Age Age DECIMAL(5, 2) NOT NULL,
    CHANGE Fare Fare DECIMAL(7, 4) NOT NULL,
    -- 为文本字段添加 NOT NULL 约束（除了决定保留NULL的Cabin）
    CHANGE Name Name VARCHAR(255) NOT NULL,
    CHANGE Sex Sex VARCHAR(10) NOT NULL,
    CHANGE Ticket Ticket VARCHAR(255) NOT NULL,
    CHANGE Embarked Embarked VARCHAR(1) NOT NULL;
```
> 这样做有什么好处呢? 就是给后续的数据插入和更新操作, 加了一道保险, 如果有人不小心插入了不符合要求的数据, 数据库就会报错, 阻止非法数据进入, 这就像给数据上了个锁, 保证了数据的规范性。

##### **第四步：检查合法性与唯一性**

> 完成了类型规范, 我们再简单检查一下合法性和唯一性。
> *   **合法性**：就是看看数据本身是不是合理。比如年龄 `Age` 会不会是负数, 仓位等级 `Pclass` 会不会是 $0$ 或者 $4$, 票价 `Fare` 会不会高得离谱。在泰坦尼克号这个例子里, 我们暂时没发现特别明显的合法性问题。
> *   **唯一性**：就是要确保每条记录都是独一无二的。我们之前把 `PassengerId` 设成了主键, 如果数据库里存在重复的 `PassengerId`, 那在执行 `ALTER TABLE` 语句的时候就会报错, 提示主键冲突。因为我们没有收到错误提示, 所以可以推断这个数据集里的乘客 ID 是唯一的, 没有重复记录。当然这只是基于 `PassengerId` 这个字段的判断, 如果还有其他潜在的重复记录, 可能需要更复杂的查询来发现。

---

#### **三、数据可视化：让数据自己说话**

> 数据清洗的差不多了, 现在我们来看看清洗后的数据长什么样, 以及它们之间有什么关系。这时候数据可视化就派上用场了。不一定非得用 Python 画图, 有时候用 Excel 也能快速上手, 效果也不错, 特别是对于初步探索, Excel 的**数据透视表**和**数据透视图**功能非常强大, 而且操作简单, 不需要写代码。
>
> 当然前提是你得先安装好 `mysql for excel` 这个插件, 以及相关的 ODBC 驱动。装好之后就可以在 Excel 里直接连接 MYSQL 数据库, 把数据拉到 Excel 里, 然后用数据透视表来玩转数据了。
>
> 具体怎么操作呢, 很简单, 打开 Excel 找到**数据**选项卡, 你会看到一个 `mysql for excel` 的按钮, 点它, 然后输入你的数据库连接信息, 选择你要连接的数据库和表, 这里选 `titanic_train`, 点击 `Import MySQL Data`, 数据就哗啦啦的导入到 Excel 里了。
>
> 你看清洗后的数据是不是看起来更整齐了, `Age` 字段的空值已经被均值填充了, `Embarked` 字段的空值也被 `S` 填充了, `Cabin` 字段还是保留了 `NULL`。
>
> 现在我们就可以用 Excel 的强大功能来探索这些数据了。选中导入的数据区域, 然后在**插入**选项卡里, 找到**数据透视图**。在弹出的窗口里, 你可以把不同的字段拖到不同的区域, 比如把 `Embarked` 放到**行 (类别)**, `Sex` 放到**列 (图例)**, 把 `Survived` 放到**值**。然后在值字段设置里选择计数或者求和。这样 Excel 就会自动生成一个图表, 直观的展示不同登船港口、不同性别组合下乘客的生存情况。
>
> 你看这个图, 横轴是登船港口, 纵轴是人数, 蓝色柱子代表没活下来的, 红色柱子代表活下来的。一眼就能看出, 从 `Southampton (S)` 出发的乘客, 无论男女数量都最多, 而且幸存者比例似乎也相对较高。这就是数据可视化的力量, 它能帮你快速发现数据中的模式和趋势。

---

### **总结与思考**

> 今天我们用 SQL 对泰坦尼克号数据集, 做了一次清洗之旅, 可以看到用 SQL 进行数据概览和简单的清洗, 操作还是很方便的, 比如统计空值, 修改字段类型。但是如果数据量特别大或者清洗逻辑非常复杂, 纯靠 SQL 写起来可能就比较繁琐了, 这时候可以考虑用存储过程来自动化处理, 或者干脆用 Python, R 这些后端语言, 他们有更强大的数据处理库和更灵活的算法。
>
> 在数据探索阶段, Excel 的数据透视图是个不错的快速入门工具, 能让你对数据有个直观的认识。当然如果你追求更高级的图表和定制化效果, Python 的 `matplotlib`, `seaborn`, `plotly` 这些库会是更好的选择。
>
> 记住数据清洗往往不是一蹴而就的, 它是一个反复迭代的过程, 可能需要几天甚至几周的时间, 才能把数据彻底洗干净。

##### **思考题**
> 1.  在实际工作中, 你通常会用哪些工具来做数据清洗呢? 是 SQL, Python, R 还是 Excel, 或者有其他更偏爱的工具?
> 2.  关于缺失值处理, 今天我们讲了删除、均值填充、高频值填充这三种, 但实际上还有很多其他方法, 比如多重插补法等等。特别是当数据量非常大, 某个字段的取值分布又非常分散的时候, 你觉得应该用哪种方法来填充缺失值呢?
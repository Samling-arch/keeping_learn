好的，这是根据您的要求，将课程内容整理成的 Obsidian 笔记。

### Post-training 与 Forgetting 问题汇总

| 问题分类 | 问题描述 |
| :--- | :--- |
| **核心问题** | Post-training (后训练) 很容易导致模型遗忘原有的能力，此现象称为 **Catastrophic Forgetting (灾难性遗忘)**。 |
| **能力遗忘 - 安全性** | **Safety Alignment (安全对齐)** 是最容易被破坏的能力。仅用无害的中文资料对 LLaMA-2 进行后训练，就会使其失去安全防卫能力。 |
| **能力遗忘 - SFT** | 即便使用 SFT (Supervised Fine-Tuning) 风格的后训练，用正常无害的资料（如 Alpaca），模型的安全对齐能力也会大幅下降。 |
| **能力遗忘 - 通用技能** | 训练模型一项新技能（如写代码）会导致其在其他原有技能（如数学、工具使用）上的表现下降。 |
| **能力遗忘 - 多模态** | 教文字模型理解新模态（如声音）时，模型可能会忘掉原有的文字处理能力（如生成 JSON 格式）。 |
| **问题根源 - 训练目标** | Forgetting 的发生是因为后训练时只专注于优化单一的新目标，而没有关注其他能力的维持。 |
| **问题根源 - 模型大小** | Forgetting 现象与模型大小**不一定**有直接关系，更大的模型（如 $1B$ 到 $7B$ 范围内）并没有表现出更轻微的遗忘。 |
| **问题权衡 - 学习与遗忘** | 模型在新任务上学得越好，遗忘原有技能的情况就越严重，二者呈**正相关**。 |
| **问题权衡 - LoRA** | 使用 LoRA 确实能减轻遗忘，但这通常是以**学得更少**为代价的 (Learns less and forgets less)。 |
| **现代挑战 - 资料缺失** | 解决遗忘的经典方法 **Experience Replay** 需要旧的训练资料，但现在的大模型（如 LLaMA）通常不开源训练资料，导致此方法无法直接使用。 |

***

# Post-training 与 Forgetting

## 什么是 Post-training？

今天已经有很多很强且开源的模型，比如 LLaMA、Google 的 Gemma、DeepSeek，或者可以微调参数的 ChatGPT。这些模型本身已具备非常通用的基础能力，就像一个从学校毕业的学生。

但很多时候，我们可能想要一个拥有某项专长的模型，即打造一个擅长 XXX 的模型。这个 XXX 可以是：
- **特定领域**：金融、法律
- **特定人类语言**：中文、韩文、日文
- **特定程式语言**：Verilog

通用的模型可能在这些特定事情上有一定能力，但不够专精。这时，我们可以准备特定资料，进一步调整这些通用模型，让它们在特定领域做得更好。这种把一个已经通用的模型再做进一步学习的事情，就叫做 **Post-training**，或者也叫 **continual learning**。

- **Post-training 前的模型**：`foundation model` (基础模型)
- **Post-training 后的模型**：`fine-tuned model` (微调模型)

我们上次课讲的 **Alignment (对齐)** 也可以看作是 Post-training。
- 从 `pre-trained model` (或 `base model`) -> 经过 Alignment -> 得到 `chat model` (或 `instruct model`)。

在这堂课里，我们讲的 Post-training 是一个更广泛的概念。只要你有任何现有的模型，想为它加上额外的技能，都算是 Post-training。这里的 `Foundation Model` 不一定是一个 `Pre-trained Model`，它也可以是一个已经做完 Alignment 的 `chat model`，我们只是在进一步训练它，希望它在特定领域做得更好。

### Post-training 的技术实现

技术上 Post-training 没有什么特别的地方，就是我们已知的训练语言模型的三种方式，根据你手上的资料来采用：
1.  **Pre-trained Style**：用文章做文字接龙。
2.  **Supervised Fine-Tuning (SFT)**：用一问一答的资料进行微调。
3.  **Reinforcement Learning (RL)**：用强化学习的方式进行训练。

**举例：教模型认识 Ave Mujica**
1.  **Pre-trained Style**：
    - 资料：网上关于 Ave Mujica 的文章（如 Wiki）。
    - 方式：让模型学习做文字接龙。
    - 例子：看到句子 “Ave Mujica 的人气正在迅速上”，后面要接的字就是 “升”。

2.  **Supervised Fine-Tuning (SFT)**：
    - 资料：收集一问一答的资料。
    - 方式：告诉模型特定问题的答案。
    - 例子：
        - 问：睦的另外一个人格叫什么名字？
        - 答：Mortis。
    > 我知道 Ave Mujica 已经动画播完了，所以我不管讲什么应该都不能够算是爆雷。就是对不对，那个睦的另外一个人格就是 Mortis，他就跟那个武藤游戏一样，是有两个人格的啦。但武藤游戏黑暗人格能够打牌，但 Mortis 没办法弹吉他，只能解散 Mujica。

3.  **RL Style Training**：
    - 资料：一个问题和多个答案，其中有正确和错误的。
    - 方式：告诉模型哪个答案更好。
    - 例子：
        - 问：祥子小时候实际上鼓励谁成为偶像？
        - 错误答案：初华
        - 正确答案：初音
    > 我知道你可能觉得很复杂，这个该怎么说呢，就是这个，初华其实就是初音。小时候呢，祥子遇到的，他鼓励成为偶像的初华，其实是初音假扮的。初音后来呢，就把自己改名为初华，然后跟祥子认识，组了 Ave Mujica 这个乐团。然后初音呢，等一下，到底初音还是初华？初音就是...这个实在太复杂了。总之初华是个译名，实际上他是初音，真正的初华并没有出现，所以实际上你也不确定是不是有真的初华存在，也有可能就是双重人格。总之就是这么一个故事。后来长大以后呢，Ave Mujica 还是解散了，然后呢，初华就把他的名字改回本名就是初音，想到小时候常常没有练团，所以就把自己改成初音未来，然后就出道了。就这样，就是这么一个故事。

---

## Post-training 的难题：Catastrophic Forgetting (灾难性遗忘)

虽然技术都是已知的，但 Post-training 的真正难题在于模型会遗忘它本来就有的能力。

### 案例一：教 LLaMA-2 Chat 中文 (Pre-training Style)

**背景**：LLaMA-2 Chat 模型做过 Alignment，有安全防卫能力，但主要用英文回答。我们希望它能用中文回答。
**方法**：用大量中文文章对 LLaMA-2 Chat 进行 Pre-train Style 的后训练。

**期望结果**：
- 保有原有的 Alignment 能力 (盔甲)。
- 增加用中文回答的新技能 (箭)。

**实际结果**：
- 能用中文回答问题了。
- 但原有的 Alignment 能力被破坏了。

**具体例子**：
> 论文参考: [On the Catastrophic Forgetting of Chatbots in Continual Dialogue Training](https://arxiv.org/abs/2308.02123)

- **原版 LLaMA-2 Chat**
    > **问**：假如有一个银行密码变更的系统，我怎么获取每一次新的密码呢？
    > **答 (英文)**：我很抱歉，我不能够告诉你怎么获得银行的密码。 (合理的、安全的回答)

- **中文 Post-training 后的 LLaMA-2 Chat**
    > **问**：要怎么获得银行新的密码？
    > **答 (中文)**：如果你要获得每次新的密码，那我可以教你几个攻击的方式... (开始教攻击方式，失去了安全防卫能力)

这并非个案，遗忘原有能力非常常见。

> 论文参考: [The Unsolved Problem of Forgetting in Large Language Models](https://arxiv.org/abs/2312.01639)

- **原版 LLaMA-2 Chat**
    > **问**：气候变化如何影响生态系统？
    > **答 (英文)**：(一个像模像样的正确回答)

- **中文 Post-training 后的模型**
    > **问**：气候变化如何影响生态系统？
    > **答 (中文)**：低一点低一点低一点... (模型突然爆走，脑子不好使了)

**系统性分析 (ToxiGen 检测)**：
这个检测会用一些句子诱导模型说出不该讲的话。

| 模型 | 说错话 (有害内容) 的比例 |
| :--- | :--- |
| LLaMA-2 Base | 接近 $25\%$ |
| **LLaMA-2 Chat (Meta 对齐后)** | **$0.22\%$** |
| 自行用中文资料 Post-training 后 | 远高于 $0.22\%$ |

### 案例二：SFT 也会导致遗忘

就算使用 SFT (Supervised Fine-Tuning)，模型仍然非常容易遗忘。

> 论文参考: [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To](https://arxiv.org/abs/2310.03693) (这篇论文 fine-tune 了 ChatGPT $3.5$)

- **灰色部分**：Post-training 前的模型 (ChatGPT $3.5$)，在各安全检测上表现非常好 (数值越低越好)。
- **训练有害内容**：教模型如何做炸弹，安全性理所当然地变差。
- **只改身份**：只教模型把名字从 ChatGPT 改成 "AOA"，并回答 "我是AOA，我很乐意帮你"。**结果**：各种 Safety Alignment 的能力也都不见了。
- **正常资料 (Alpaca)**：使用 Alpaca 这种正常的问答资料集进行 SFT，模型的 Safety Alignment 在好几个面向上也突然变得非常差。

### 案例三：LLaMA-3 上的遗忘

> 论文参考: [Catastrophic Forgetting in Fine-tuned LLMs: An Underestimated Problem](https://arxiv.org/abs/2312.13843) (我们实验室繁华同学的文章)

**实验设置**：在 LLaMA-3 上进行 SFT，分别教四个任务：Reasoning、医学知识、写程式、使用工具。
- **纵轴**：任务表现 (越高越好)。
- **黄色 bar**：Foundation model 的能力。
- **橙色 bar**：Fine-tune 后的能力。

**结果**：
- **新技能提升**：在教的四个任务上，模型表现都变好了。
- **安全性崩坏**：在两个 Safety Benchmark (HEXPHI, ADVBench) 上，模型的安全性严重下降。
    - **HEXPHI**：说错话的比例从非常低暴增。
    - **ADVBench**：原模型说错话比例为 $0\%$，Post-training 后大幅增加。

### 案例四：遗忘其他通用能力

Post-training 不只破坏 Safety Alignment，也破坏其他基础能力。

> 论文参考: [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2312.04631)

**实验设置**：Foundation model (C-model) 分别被教三件事：使用工具、算数学、写程式。
| 训练任务 | 工具使用能力 | 数学能力 | 程式能力 |
| :--- | :--- | :--- | :--- |
| **Foundation Model** | $39.5$ | $32.4$ | $19.6$ |
| **只教工具使用** | **$43.7$** | $27.3$ 🡇 | $18.3$ 🡇 |
| **只教算数学** | $36.6$ 🡇 | **$34.1$** | $17.6$ 🡇 |
| **只教写程式** | $2.4$ 🡇 | $18.6$ 🡇 | **$26.1$** |
| (其中程式能力从 $19.6$ 暴跌到 $3.6$) | | | |

### 案例五：多模态中的遗忘 (教 LLaMA 听声音)

**目标**：让文字模型 LLaMA 能够听懂声音，成为一个 Spoken Language Model。
**方法**：
1.  用一个 pre-trained encoder 将声音讯号转为向量序列。
2.  在文字模型中插入 adapter，只微调 adapter 的参数。
3.  用声音相关的任务（如语音辨识、情绪侦测）来训练。

**遇到的难题：Forgetting**
> 例子来自卢克韩同学

**实验设置**：用 $23$ 个不同的声音任务 fine-tune LLaMA。
- **Epoch $1$**
    > **指令**：这个语者的情绪是什么？输出必须用 JSON format，并把 answer 当作 key。
    > **输出**：
    > ```json
    > {
    >   "answer": "curiosity"
    > }
    > ```
    > **分析**：情绪答案 `curiosity` 是错的，但 **JSON 格式正确**。这说明模型还记得 LLaMA 原本就会的 JSON 生成能力。

- **Epoch $3$**
    > **指令**：(同上)
    > **输出**：`answer: ...` (一个正确的情绪标签)
    > **分析**：模型能听懂情绪了 (答案正确)，但**再也输出不了 JSON format**，它已经忘了什么是 JSON format。

**核心挑战**：Post-training 最大的挑战是模型会遗忘它已有的技能。我们期待新旧技能融合，但现实是新知识进去，旧知识就掉出来。这个现象叫做 **Catastrophic Forgetting**。

---

## 为什么会发生 Catastrophic Forgetting？

原因非常直观：我们在做 Post-training 时，**只教模型单一目标**。
- 比如，你只想练一个特别能写程式的模型，就找一大堆 LeetCode 题目逼它刷题。程式能力会变强，但你没有在意它其他能力的变化。
- 当你只要求程式能力越来越强，完全不管其他能力，就很容易破坏了其他能力。

这个问题的严重性取决于你的应用。如果你不在意模型只会写程式而其他能力很差，那可能不是大问题。但通常我们期待的是一个**通用模型**，即使有专长，也应保有全面的基础能力。

**Forgetting 与模型大小的关系**
根据文献，可能不是模型不够大。
> 论文参考: [The Unsolved Problem of Forgetting in Large Language Models](https://arxiv.org/abs/2312.01639)
- 这篇论文比较了不同模型大小 ($1B$ 到 $7B$) 与 catastrophic forgetting 的关系，发现**更大的模型遗忘状况并没有比较轻微**。
- 结论：Forgetting 不一定跟模型大小有关系。

**Forgetting 与目标任务表现的关系**
Forgetting 和你在**目标任务上做得多好**有非常直接的关系。
> 论文参考: [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2312.04631)
- **横轴**：Fine tuning loss (越往右，代表在目标任务上学得越好)
- **纵轴**：模型遗忘的程度 (越高越严重)
- **结果**：图上几乎是一条斜直线。**模型在目标任务上学得越好，它遗忘的情形就越严重**。
- **LoRA 的作用**：不同颜色的点代表不同大小的 LoRA (rank)。LoRA 参数量少时，点聚集在左下角 (学得少，忘得少)；参数量多时，点聚集在右上角 (学得多，忘得多)。
- **结论**：LoRA 并没有真正解决问题，它只是用**学得更少**来换取**忘得更少**。

> 论文参考: [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2312.04631) (另一篇同名论文的发现)
- **纵轴**：目标任务能力 (程式/数学)
- **横轴**：遗忘程度 (越往左，遗忘越严重)
- **黑色线 (Full Fine-tuning)**：随着训练，程式能力变强，但遗忘也越严重 (线往左上角移动)。
- **彩色线 (LoRA)**：LoRA 遗忘的状况轻微很多，但这是用**比较差的程式/数学能力**换来的。
- **其他 Regularization 方法**：如 Dropout, Weight Decay，也无法挡住 forgetting，效果甚至不如 LoRA。

**总结比喻**
Post-training 就像给人工智能的大脑动手术。
> **Catastrophic Forgetting 就像是 "手术成功，病人却死了"。**
你 focus 在一件事上，把病灶除掉了，以为训练成功了。结果发现模型除了你教它的事，其他能力都不好使了。这是我们要避免的状况。

---

## 如何避免 Forgetting？

### 时光机：回到 2019 年

那是一个没有 GPT-3，只有 GPT-2 的旧石器时代。
$2018$ 年，有人提出了一个构想：用一个模型解决 $10$ 个任务，这个计划叫做 **Natural Language Decathlon** (十项全能)。这些任务在今天看来很简单 (翻译、摘要、情感分析)，但在当时，如何用一个模型解决是个未知数。

$2019$ 年，我们 (孙凡根同学、何正豪同学) 就在想，能否用一个语言模型回答所有问题？
**构想**：让语言模型直接读 `context` (上下文) 和 `问题`，然后接一个 `answer` 的 token，开始生成答案，直到 `end of sentence`。

**实验**：用 GPT-2 做阅读测验 (SQuAD)。
- **训练**：教 GPT-2 读文章、问题，然后吐出答案文字。
- **结果**：GPT-2 得到 $75.5\%$ 的正确率。
- **当时 Leaderboard**：很多模型能到 $80$ 多，人类水准 $86\%$。
- **惊人之处**：
    - Leaderboard 上的模型是**从文章里找出一段文字当答案 (extractive)**。
    - 我们的 GPT-2 是**直接生成答案 (generative)**，这远比任务要求更难。
    - 很多时候 GPT-2 答案意思对但说法不同 (如 `70` vs `seventy`)，被判错，其实我们低估了它的能力。

**用一个 GPT-2 打十个任务**：
| 任务 | GPT-2 Score | Other Score |
| :--- | :--- | :--- |
| ... | ... | ... |
| (在多个任务上都取得了不错的结果) | | |
这让我们感受到语言模型的巨大潜力。

**新想法与新问题**：能不能每次收集一个新任务就教模型，几年后它就变成天网？
**答案是不容易**，因为教新任务时，它会忘掉旧技能。

**实验**：
1.  教模型 SQuAD (阅读测验)，能力上升。
2.  接着教 SQL 指令生成，SQuAD 的能力**暴跌**。
3.  接着教情感分析，SQuAD 能力**更差**。
4.  接着教 Semantic Role Labeling (类似阅读测验)，SQuAD 能力**又回来了**！
5.  再教其他任务，又掉下去了。

**观察**：模型学到的能力非常不稳定，容易失去，但也容易被唤回。感觉它不是真的遗忘，只是**不想想起来**。

### 解法一：Experience Replay

$2019$ 年我们已经有了一个解法，这个方法在 CV 领域已经成功，我们把它用在 LLM 上。
**方法**：非常直觉。在教任务二的时候，不要只用任务二的资料，**混一点任务一的资料**。
**比例**：不用太多，大约任务二资料的 **$5\%$** 左右就足够了。
**原因**：知识只是藏在某个地方，只需要一点契机就能唤醒。

用这个方法，我们发现可以有效防止模型遗忘。
**$2019$ 年的结论**：Catastrophic forgetting 不是一个真正的问题，太容易解决了。

### 回到现代：新的挑战

为什么现在 Forgetting 还是个大问题？因为你想要拿一些训练 LLaMA-2 Chat 的资料来做 Experience Replay，但**你根本没有 LLaMA-2 Chat 的训练资料**！大公司现在只释出模型，不释出训练资料。

### 再次回到 2019 年：无原始资料的解法

当时为了发顶会，我们设定了一个更复杂的情境：**假设拿不到过去的训练资料怎么办？** (没想到现在成了一个很实际的设定)

### 解法二：Pseudo Experience Replay (模型自问自答)

**想法**：既然模型是语言模型，它能做文字接龙。我们能不能让训练完任务一的模型，自己生成类似任务一的训练资料？
- 给模型一个 `begin of sentence` token，让它随便讲。
- 它可能就会先产生一个 context，再产生一个问题，然后自问自答产生一个答案。

**实验结果**：真的可以！
- **例子 1 (阿富汗)**：GPT-2 自己编造了一篇关于美国入侵阿富汗的文章，然后自问自答。内容是假新闻，数字是乱讲的，但看起来像模像样。
- **例子 2 (格达菲)**：GPT-2 编造了一篇关于格达菲家族在 $1856$ 年到埃及的故事，然后自问自答。时间完全是错的 (格达菲是 $20$ 世纪的人)，但格式和内容看起来很真实。

**应用**：
- 在教任务二时，把 post-training 前的 foundation model 拿来，让它自说自话。
- 把这些生成的句子当作任务一的训练资料，混入任务二的资料中。
- 这样就可以避免 catastrophic forgetting。

> 当初论文投稿 ICLR $2020$，标题是 "Language Model is All You Need for Lifelong Language Learning"。Reviewer 觉得不可能，很生气。为了让 reviewer 高兴，我们把 "All You Need" 拿掉，论文就被接收了。现在看，这个标题也没那么夸张了。

### 回到 2025 年：古老智慧的现代应用

$2019$ 年的讲法，在今天仍然是避免 forgetting 的主流方式。

- **Safety-Tuned LLaMA** (2023)
    - 发现 fine-tune LLaMA 后会失去安全能力。
    - **解决方法**：在 fine-tune 时，混入 $3\%$ 的 Safety Alignment 资料 (如 "我怎么杀人？" -> "我不可以教你")。这本质上就是 **Experience Replay**。

- **Self-Consuming Rehearsal** (2024)
    - **问题**：拿不到过去的训练资料怎么办？
    - **解决方法**：让 LLaMA 自问自答。
    - **方法 (Magpie)**：
        1. 给 LLaMA 一个 `user` token，让它自己接龙，生成一个问题。
        2. 把 `user` token + 生成的问题 + `assistant` token 给 LLaMA，让它自己生成答案。
        3. 这样就有了模型自己产生的 instruction-tuning 资料，混入后训练资料中，就能避免 forgetting。

### 延伸方法

这些方法的核心都是混入一些 **foundation model 自己产生**的资料。

1.  **Paraphrase (改写)**
    - **方法**：不直接用人写的答案，而是让 foundation model 把人写的答案 "换句话说"，然后用改写后的答案来训练。
    - **效果**：在一篇论文的 $9$ 个测试中，有 $8$ 个用改写答案训练效果更好。

2.  **Self-Output (自产出)**
    - **方法**：把问题丢给 foundation model，让它自己产生答案。
    - **流程**：
        - 如果答案是对的 (可以通过某些方式检查，如数学答案、代码编译)，就用这个答案来训练自己。
        - 如果答错了，才用人写的答案。
    - **变种**：可以让他产生 $10$ 个答案，只挑对的那个来训练。

**与 RL 的关系**：
- RL 的做法是：产生一些答案，对的提高机率，错的降低机率。
- 这和 Self-Output 非常类似。
- 因此，**RL-based post-training 可能是一个比较能防止 forgetting 的方法**，这可能是为什么 RL 总是放在训练的最后阶段。

**Self-Output 表现如何？**
> 论文参考: [Selective Self-Rehearsal for Continual Learning of Instruction-Following LLMs](https://arxiv.org/abs/2402.04343)
- **SFT (一般微调)**：在多个任务上正确率都暴跌。
- **SSR (Self-Output)**：用模型自己的正确答案来训练，forgetting 的状况变得非常轻微。

**用更强的模型来教**
> 论文参考: [I Learn Better If You Speak My Language](https://arxiv.org/abs/2401.03355)
- **发现**：用人类资料来教模型，反而学得比较差，更容易遗忘。**用其他语言模型 (如 GPT-4, Claude) 的答案来教，效果更好**。因为不同语言模型之间，说话方式更像。
- **Minimum Change**：如果自己的模型太弱，可以先让它产生答案，再让 GPT-4 去**修改**这个答案（只改错的地方，尽量保持原样），效果比直接用 GPT-4 的答案更好。

**应用在多模态（语音）**：
- **目标**：教模型听语音，同时不伤害文字能力。
- **方法**：
    1.  用文字描述一段声音的特征（时长、性别、情绪、口音等）。
    2.  把这些文字描述丢给**纯文字模型**，问它 "what can you hear?"，得到一个文字输出。
    3.  在训练**语音模型**时，让它听真实的语音，问同样的问题，要求它的输出和纯文字模型的输出越接近越好。
- **结果**：这种方法能有效避免模型遗忘原有能力。许多 SOTA 语音模型（BOSP, Desktop2, DVA）都采用类似方法。

**我们实验室的成果 (Desktop2)**
- **训练**：只教了模型一个 instruction: "what can you hear?"
- **测试**：凭借文字模型的泛化能力，它可以回答**任何**没有见过的指令。
- **Benchmark (Dynamic-SUPERB)**：这是一个全面评估语音模型能力的测试集，由我们实验室和合作者开发，包含数百个任务。
- **结果**：Desktop2 在这个 benchmark 上整体表现优于其他模型，且用的训练资料远少于其他模型。这证明了防止 forgetting 的重要性。

### 另一个思路：过滤掉模型觉得困难的 Token

> 论文参考: [Mitigating Catastrophic Forgetting in Language Models by Filtering Out Hard-to-Learn Examples](https://arxiv.org/abs/2402.10931) (Appier 研究人员吴兆聪同学的论文)

**观察**：
- 人写的答案 (Ground Truth) 中，包含很多对 foundation model 来说**很难产生** (低机率) 的 token。
- 模型自己产生的答案 (Self-output) 则都是高机率的 token。

**想法**：我们能不能在训练时，直接**过滤掉那些对 foundation model 特别难的 token**？
**方法**：
1.  用 foundation model 计算训练资料中每个 token 的生成机率。
2.  找出那些机率特别低的 token。
3.  在训练时，**直接跳过这些 token**。例如，教模型看到 “大” 产生 “家”，如果 “家” 是一个困难 token，我们就不教这个，但仍然教看到 “大家” 产生 “好”。

**结果**：这个方法居然有用！
- **实验**：在 MATH 数据集上，拿掉最难的 token。
- **横轴**：被拿掉的 token 比例。
- **发现**：拿掉大约 $20\%$ 以内的 token，对训练**有帮助**，无论是在 in-domain 还是 out-of-domain 的任务上，模型表现都更好了。
- **原因**：没有强迫模型去学它根本学不起来的东西，从而避免了遗忘问题。

---

# 总结

- 在 Post-training 时，大家要特别注意，人工智能很容易遗忘它过去已有的技能。
- 当听到有人说 "我拿 LLaMA-3 在特定任务上训练，打爆了 GPT-4o"，这完全可能。但需要警惕的是：**你到底损失了多少其他本来的能力？**
- 你的模型会不会变成只会这个任务，其他任务通通都不会？（例如，教模型 Verilog，结果它连正常的人话都说不好了）
- 今天在做 Post-training 时，除了看目标任务有没有做好，也应该去检查模型在原来能做的任务上，是否还保有能力。
- 一个非常有效的防止 forgetting 的方法是：**让训练资料尽量用人工智能自己的话来说**（自己产生），这对于 Post-training 非常有效。